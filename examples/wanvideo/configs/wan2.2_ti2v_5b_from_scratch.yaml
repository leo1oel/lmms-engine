trainer_type: wan_trainer
# Dataset configuration
dataset_config:
  dataset_type: vision
  dataset_format: jsonl
  dataset_path: data/example_video_dataset/metadata.jsonl
  video_sampling_strategy: frame_num
  frame_num: 49
  shuffle: false
  video_backend: qwen_vl_utils
  # Processor configuration
  processor_config:
    processor_name: WanVideo/Wan2.1-T2V-3B
    processor_type: wanvideo
    extra_kwargs:
      do_resize: true
      size:
        height: 480
        width: 832
      do_normalize: true
      image_mean: [0.5, 0.5, 0.5]
      image_std: [0.5, 0.5, 0.5]
      
model_config:
  load_from_config:
    model_type: wanvideo
    config:
      # DiT model parameters
      dit_hidden_size: 3072
      dit_num_layers: 30
      dit_num_heads: 24
      dit_intermediate_size: 14336
      dit_patch_size: [1, 2, 2]
      dit_in_channels: 48
      dit_out_channels: 48
      dit_freq_dim: 256
      dit_text_dim: 4096
      dit_eps: 1.0e-6
      dit_has_image_input: False

      dit_has_image_pos_emb: False
      dit_has_ref_conv: False
      dit_add_control_adapter: False
      dit_in_channels_control_adapter: 24
      seperated_timestep: True
      require_clip_embedding: False
      require_vae_embedding: False
      fuse_vae_embedding_in_latents: True
      trainable_modules: 'dit'
  
# Training arguments
trainer_args:
  output_dir: "./output/wan2.2_ti2v_5b"
  num_train_epochs: 10000 # you dont have to set it like this, just for demo
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4  # Higher for 14B model
  gradient_checkpointing: true
  
  learning_rate: 5.0e-6  # Lower LR for larger model
  weight_decay: 0.01
  warmup_steps: 1000
  lr_scheduler_type: "cosine"
  
  logging_steps: 10
  save_steps: 250
  save_total_limit: 1
  
  dataloader_num_workers: 4
  run_name: lmms_engine_wan2.2_ti2v_3b_demo_test
  fp16: false
  bf16: true
  tf32: true
  
  seed: 42
  remove_unused_columns: false
  # Optimization
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  
  sp_ulysses_degree: 1
  
  deepspeed: examples/ds_config/default_config.json
  
      # FSDP configuration for multi-GPU training
      # fsdp: "full_shard auto_wrap"
      # fsdp_config:
      #   backward_prefetch: "backward_pre"
      #   forward_prefetch: true
      #   use_orig_params: false
      #   cpu_ram_efficient_loading: true
      #   sync_module_states: true
      #   limit_all_gathers: true
      #   activation_checkpointing: true
  