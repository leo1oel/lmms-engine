# Unified LMMs Engine Training Configuration
# This example shows how to combine dataset definitions directly in the config

- type: trainer
  config:
    trainer_type: hf_trainer
    
    # Dataset configuration - now includes the actual dataset definitions
    dataset_config:
      dataset_type: vision
      dataset_format: yaml  # Uses 'yaml' format for both external files and inline definitions
      
      # Inline dataset definitions (no dataset_path needed)
      datasets:
        - json_path: data/open_thoughts_debug
          data_folder: ""
          data_type: arrow
      
      # Processor configuration
      processor_config:
        processor_name: "Qwen/Qwen2.5-VL-7B-Instruct"
        processor_type: "qwen2_5_vl"
      
      # Packing configuration
      packing: true
      packing_strategy: first_fit
      packing_length: 20480
    
    # Model configuration
    model_config:
      model_name_or_path: "Qwen/Qwen2.5-VL-7B-Instruct"
      attn_implementation: "flash_attention_2"
    
    # Training arguments
    per_device_train_batch_size: 1
    learning_rate: 1e-06
    weight_decay: 0.0
    gradient_accumulation_steps: 1
    gradient_checkpointing: true
    num_train_epochs: 1
    save_steps: 100
    save_total_limit: 1
    report_to: "none"
    output_dir: "./output/debug"
    warmup_ratio: 0.0
    run_name: "qwen2_5_vl_config"
    eval_strategy: "no"
    logging_steps: 1
    group_by_length: true
    dataloader_num_workers: 8
    bf16: true
    lr_scheduler_type: "cosine"
    freeze_modules: ["visual"]
    use_liger_kernel: true
    use_rmpad: true
    fsdp2: true
    fsdp_config:
      transformer_layer_cls_to_wrap: ["Qwen2_5_VLDecoderLayer"]
      reshard_after_forward: false